# 第 9 回： [DALL·E の理解に向けて part 1] Vision Transformer の論文を読んだ

収録日：20210110

発表者：[@yohei_kikuta](https://twitter.com/yohei_kikuta)  

内容：
- DALL·E とは？
  - OpenAI が発表した caption text から実に多様なコンセプトの画像を生成するモデル
  - 論文はまだ発表されていないが、公式サイトで例示された生成画像が驚きの凄さだった
  - この機会に Vision Transformer なども理解し、シリーズ物として配信してみることにした
- Vision Transformer (ViT) の論文紹介
  - モチベーション
    - モチベーションは特に面白いとこはない。NLP で非常にうまくいってる Transformer 型モデルを vision でも適用したいというもの
    - NLP においては興味深いべき乗則を示しているので、それが vision においても成立するかはモチベーションの一つ
    - 同一の Transformer 型モデルで両方扱えるようになればマルチモーダルに色々やれそうというのもあるが、その辺はあまり書いてない
  - モデルの説明
    - 基本的な構造は Transformer Encoder (BERT) と同じ
    - 入力は画像を patch 化 → flatten → embedding して、これを positional embedding と足す
      - NLP における token ↔︎ ViT における patch (典型的には 16x16x3 とする)
    - 文頭に [CLS] token を入れるところも同じ
    - pre-training は unsupervised ではなくて教師ラベル付きのデータを使って supervised で実施
      - unsupervised だと思ってた！（だからこそ大規模 pre-training ができると思ってたので）
      - ImageNet とか Google 内部の外部非公開データ JFT-300M で supervised pre-training をしている
      - unsupervised な手法も試しているけど downstream タスクの性能は明確に悪い
        - mask token の予測として patch の RGB 空間平均を (R, G, B) = ([0..7], [0..7], [0..7]) で 512 クラス分類問題として解く
        - 他にも pixel 差分を l2-loss で解くとかも試している
        - いかにもイマイチという感じだが、実際にこれらでは大した性能が出ず、論文では supervised を採用
        - PyTorch 実装では Bootstrap Your Own Latent という手法で pre-training できるようになってる（その結果どれくらい良くなるかは書いてない）
  - 性能
    - pre-training してから、CLS token の後の MLP を差し替えて downstream の画像分類タスクを解いて評価
    - 先行研究で最も高い性能を示していた BiT や Noisy Student を使って EfficientNet と同等以上の性能
    - TPUv3-core-days で学習時間を比較したときに、先行研究の手法よりも 1/4 ほどの時間しか掛かっていない
      - これがなんでなのかパッとは分からない...論文にもちゃんと書いてない...誰か教えて欲しい...
    - 興味深いのは、データサイズが小さいうちは明らかに先行研究の手法に劣っているが、データサイズを 1000 万枚以上にすると同等以上の性能を発揮する（データ数に対するスケーリングが他の手法より強力なのかも）
    - convolution の効果を調べるために、ResNet で feature map を抽出したものを flatten → embedding するという hybrid モデルも試しているが、これもデータが少ないうちは convlution によって性能が上がるけどデータが多くなると特に効果がなくなる
      - 十分なデータがあれば convolution で明示的に入れていたような inductive bias も学習してくれそう
    - 他にも embedding は 2D 構造とか持たせずシンプルに 1D でよさそうとか、色々実験をしている
  - 応用可能性に関して
    - この論文では色々実験してちゃんと vision でも Transformer 型モデルが有効そうというのを明らかにした（good paper ではあると思うけど great paper ではない）
    - これだけ NLP の Transformer 型モデルと近しい形で扱えるなら、当然それを組み合わせてマルチモーダルで強力なことができそう
      - それをやった筆頭候補が DALL·E なので、論文が出るのが楽しみですな
  - ICRL 2021 に投稿中
    - 数日後に Paper Decision Notification で、reviewer 全員 accept なので通りそう
    - reviewer が誰も自分が気にしてる学習効率性の観点に突っ込んでないんだよな...

参考情報：

- DALL·E：https://openai.com/blog/dall-e/
- An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale：https://arxiv.org/abs/2010.11929
- 菊田が論文を読んだときのメモ：https://github.com/yoheikikuta/paper-reading/issues/56
- 公式実装：https://github.com/google-research/vision_transformer
- PyTorch 実装：https://github.com/lucidrains/vit-pytorch
- Bootstrap your own latent: A new approach to self-supervised Learning：https://arxiv.org/abs/2006.07733
